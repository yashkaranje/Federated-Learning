{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\Yash\\AppData\\Roaming\\Python\\Python36\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yash\\AppData\\Roaming\\Python\\Python36\\site-packages\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from data_processing import LoadData\n",
    "from model import *\n",
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook(torch)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    \n",
    "    dataset = pd.read_csv('PeMS_04/PeMS04.csv')\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split1 = int(np.floor(0.6 * dataset_size)) #60-40 Bob-Alice\n",
    "    bobs_indices, alices_indices = indices[:split1], indices[split1:]\n",
    "\n",
    "    bobs_sampler = SubsetRandomSampler(bobs_indices)\n",
    "    alices_sampler = SubsetRandomSampler(alices_indices)\n",
    "\n",
    "    train_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "                              time_interval=5, history_length=6,\n",
    "                              train_mode=\"train\")\n",
    "\n",
    "\n",
    "    bobs_loader = DataLoader(train_data, batch_size=64, num_workers=32, sampler=bobs_sampler)\n",
    "    alices_loader = DataLoader(train_data, batch_size=64, num_workers=32, sampler=alices_sampler)\n",
    "\n",
    "#     # Loading Dataset\n",
    "#     train_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "#                           time_interval=5, history_length=6,\n",
    "#                           train_mode=\"train\")\n",
    "\n",
    "#     train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=32)\n",
    "#....................................................\n",
    "#     test_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "#                          time_interval=5, history_length=6,\n",
    "#                          train_mode=\"test\")\n",
    "\n",
    "#     test_loader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=32)\n",
    "    \n",
    "#     for data in train_loader: \n",
    "#         print(data)\n",
    "\n",
    "    #model = GCN(in_c=6 , hid_c=6 ,out_c=1)\n",
    "    #ChebNet(in_c=6, hid_c=32, out_c=1, K=2)\n",
    "    #GATNet(in_c=6 , hid_c=6 ,out_c=1, n_heads=1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bobs_model = GATNet(in_c=6 , hid_c=6 ,out_c=1, n_heads=1)     # 2阶切比雪夫模型    \n",
    "    bobs_model = bobs_model.to(device)\n",
    "    \n",
    "    alices_model = GATNet(in_c=6 , hid_c=6 ,out_c=1, n_heads=1)      # 2阶切比雪夫模型\n",
    "    alices_model = alices_model.to(device)\n",
    "\n",
    "    #new\n",
    "    bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "    alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "    secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")\n",
    "\n",
    "    #new\n",
    "#     bobs_model = model.copy().send(bob)\n",
    "#     alices_model = model.copy().send(alice)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    #new\n",
    "    bobs_opt = optim.Adam(params=bobs_model.parameters())\n",
    "    alices_opt = optim.Adam(params=alices_model.parameters())\n",
    "\n",
    "    # Train model\n",
    "    Epoch = 8\n",
    "\n",
    "    bobs_model.train()\n",
    "    alices_model.train()\n",
    "    for epoch in range(Epoch):\n",
    "    \t#new\n",
    "        epoch_loss_bob = 0.0\n",
    "        epoch_loss_alice = 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for data in bobs_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n",
    "            bobs_model.zero_grad()\n",
    "            predict_value = bobs_model(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            epoch_loss_bob += loss.item()\n",
    "            loss.backward()\n",
    "            bobs_opt.step()\n",
    "            \n",
    "        for data in alices_loader:\n",
    "            alices_model.zero_grad()\n",
    "            predict_value = alices_model(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            epoch_loss_alice += loss.item()\n",
    "            loss.backward()\n",
    "            alices_opt.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"Epoch: {:04d}, Loss Bob: {:02.4f}, Loss Alice: {:02.4f}, Time: {:02.2f} mins\".format(epoch, 1000 * epoch_loss_bob / len(train_data), 1000 * epoch_loss_alice / len(train_data),\n",
    "                                                                          (end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, Loss Bob: 0.0381, Loss Alice: 0.0241, Time: 3.43 mins\n",
      "Epoch: 0001, Loss Bob: 0.0374, Loss Alice: 0.0272, Time: 2.66 mins\n",
      "Epoch: 0002, Loss Bob: 0.0388, Loss Alice: 0.0276, Time: 2.69 mins\n",
      "Epoch: 0003, Loss Bob: 0.0397, Loss Alice: 0.0296, Time: 3.03 mins\n",
      "Epoch: 0004, Loss Bob: 0.0352, Loss Alice: 0.0227, Time: 2.72 mins\n",
      "Epoch: 0005, Loss Bob: 0.0379, Loss Alice: 0.0283, Time: 2.57 mins\n",
      "Epoch: 0006, Loss Bob: 0.0347, Loss Alice: 0.0234, Time: 3.24 mins\n",
      "Epoch: 0007, Loss Bob: 0.0383, Loss Alice: 0.0275, Time: 2.87 mins\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
